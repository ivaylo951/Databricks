{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2051ec-58b9-49a9-a820-314dd9672d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),        # Log to console\n",
    "        logging.FileHandler('silver_layer.log')  # log to a file \n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set catalog and schema names\n",
    "bronze_catalog = \"bronze\"\n",
    "bronze_schema = \"training_io\"\n",
    "silver_catalog = \"silver\"\n",
    "silver_schema = \"training_io\"\n",
    "\n",
    "# Set table names\n",
    "bronze_teams = f\"{bronze_catalog}.{bronze_schema}.teams\"\n",
    "bronze_players = f\"{bronze_catalog}.{bronze_schema}.players\"\n",
    "bronze_goals = f\"{bronze_catalog}.{bronze_schema}.goals_data\"\n",
    "\n",
    "silver_teams = f\"{silver_catalog}.{silver_schema}.teams\"\n",
    "silver_players = f\"{silver_catalog}.{silver_schema}.players\"\n",
    "silver_goals = f\"{silver_catalog}.{silver_schema}.goals\"\n",
    "\n",
    "def upsert(source_df, silver_table, key_columns, timestamp_column=\"load_ts\"):\n",
    "    \"\"\"\n",
    "    Perform SCD1 upsert (merge) into a Delta table using load_ts timestamp comparison.\n",
    "\n",
    "    Parameters:\n",
    "    - source_df:        Source Spark DataFrame\n",
    "    - silver_table:     Target Delta table name\n",
    "    - key_columns:      List of key columns to match records\n",
    "    - timestamp_column: Name of the timestamp field used for freshness comparison (default: 'load_ts').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build the join condition for merge\n",
    "        join_condition = \" AND \".join([\n",
    "            f\"target.{col} = source.{col}\" for col in key_columns\n",
    "        ])\n",
    "\n",
    "        # Set values to update from source\n",
    "        update_set = {col: f\"source.{col}\" for col in source_df.columns}\n",
    "\n",
    "        if spark.catalog.tableExists(silver_table):\n",
    "            delta_table = DeltaTable.forName(spark, silver_table)\n",
    "\n",
    "            logger.info(f\"Merging into existing table: {silver_table}\")\n",
    "\n",
    "            delta_table.alias(\"target\").merge(\n",
    "                source_df.alias(\"source\"),\n",
    "                join_condition\n",
    "            ).whenMatchedUpdate(\n",
    "                condition=f\"source.{timestamp_column} > target.{timestamp_column}\",\n",
    "                set=update_set\n",
    "            ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "            logger.info(f\"Upsert for table {silver_table} is completed!\")\n",
    "\n",
    "        else:\n",
    "            logger.info(f\"Creating new table: {silver_table}\")\n",
    "            source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during upsert operation for table {silver_table}: {e}\")\n",
    "\n",
    "def overwrite(source_df, silver_table):\n",
    "    \"\"\"\n",
    "    Overwrite the entire Delta table with new data.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_df: Source DataFrame to overwrite the table\n",
    "    - silver_table: Target Delta table name to be overwritten\n",
    "    \"\"\"\n",
    "    try:\n",
    "        source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "        logger.info(f\"Overwrite for table {silver_table} is completed!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during overwrite operation for table {silver_table}: {e}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Read data from bronze layer\n",
    "        bronze_teams_df = spark.read.table(bronze_teams)\n",
    "        bronze_players_df = spark.read.table(bronze_players)\n",
    "        bronze_goals_df = spark.read.table(bronze_goals)\n",
    "\n",
    "        # Call the upsert function for teams\n",
    "        upsert(bronze_teams_df, silver_teams, [\"id_team\"])\n",
    "\n",
    "        bronze_players_df = bronze_players_df.select(\"id_player\",\n",
    "            \"player_name\",\n",
    "            \"nationality\",\n",
    "            \"field_position\",\n",
    "            \"id_team\",\n",
    "            \"player_image\",\n",
    "            \"bio.age\",\n",
    "            \"bio.height\",\n",
    "            \"bio.weight\",\n",
    "            \"load_ts\"\n",
    "        )\n",
    "\n",
    "        upsert(bronze_players_df, silver_players, [\"id_player\"])\n",
    "\n",
    "        overwrite(bronze_goals_df, silver_goals)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main function: {e}\")\n",
    "\n",
    "# Run main\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b82c55-0e18-4b79-9e1c-18a1e44bd1f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # If the merge executed, check the result of the operation\n",
    "# delta_table = DeltaTable.forName(spark, silver_teams)\n",
    "# history_df = delta_table.history(1)  # Get the most recent operation\n",
    "\n",
    "# # Get the operation metrics for the most recent change\n",
    "# operation_metrics = history_df.select(\"operationMetrics\").collect()[0][0]\n",
    "# print(operation_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
