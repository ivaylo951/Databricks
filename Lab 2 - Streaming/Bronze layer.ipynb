{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ba87ca-00c9-47f2-9c25-a3ee6f6c99a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from typing import NamedTuple\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a56f0a74-e70c-413e-b0e7-d5f32fddead1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "bronze_table = \"bronze.training_io.rt_marketing\"\n",
    "checkpoint_path = \"/Volumes/bronze/checkpoints/realtime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541c22b2-6579-4bba-82a9-04292ea01ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Structured Type for Kafka Credentials ---\n",
    "class KafkaCredentials(NamedTuple):\n",
    "    username: str\n",
    "    password: str\n",
    "    keystore_password: str\n",
    "    private_key_password: str\n",
    "    truststore_password: str\n",
    "    broker: str = \"detrainingbg.adastragrp.com:9092\"\n",
    "    topic: str = \"marketing.uk.netflix\"\n",
    "    truststore_location: str = \"abfss://source-data@sttrwesteu001.dfs.core.windows.net/cert/kafka.truststore.jks\"\n",
    "    keystore_location: str = \"abfss://source-data@sttrwesteu001.dfs.core.windows.net/cert/kafka.keystore.jks\"\n",
    "\n",
    "# Function to retrieve secrets\n",
    "def get_secrets(scope: str, username: str, password: str, key_store: str, private_key: str, trust_store: str) -> KafkaCredentials:\n",
    "    \"\"\"\n",
    "    Retrieve Kafka credentials and SSL-related passwords from a Databricks secret scope.\n",
    "\n",
    "    Args:\n",
    "        scope (str): The name of the Databricks secret scope.\n",
    "        username (str): The key for the Kafka username.\n",
    "        password (str): The key for the Kafka password.\n",
    "        key_store (str): The key for the SSL keystore password.\n",
    "        private_key (str): The key for the SSL private key password.\n",
    "        trust_store (str): The key for the SSL truststore password.\n",
    "\n",
    "    Returns:\n",
    "        KafkaCredentials: A named tuple containing all Kafka-related credentials and passwords.\n",
    "    \"\"\"\n",
    "    return KafkaCredentials(\n",
    "        username=dbutils.secrets.get(scope, username),\n",
    "        password=dbutils.secrets.get(scope, password),\n",
    "        keystore_password=dbutils.secrets.get(scope, key_store),\n",
    "        private_key_password=dbutils.secrets.get(scope, private_key),\n",
    "        truststore_password=dbutils.secrets.get(scope, trust_store)\n",
    "    )\n",
    "\n",
    "# Retrieve Kafka credentials from secret scope\n",
    "def retrieve_my_kafka_credentials() -> KafkaCredentials:\n",
    "    \"\"\"\n",
    "    Retrieve Kafka credentials and SSL passwords using hardcoded secret scope and keys.\n",
    "\n",
    "    Returns:\n",
    "        KafkaCredentials: The retrieved credentials.\n",
    "    \"\"\"\n",
    "    return get_secrets(\n",
    "        scope=\"dbc-training-scope\",\n",
    "        username=\"KafkaUsername\",\n",
    "        password=\"KafkaPassword\",\n",
    "        key_store=\"KafkaSSLKeystorePassword\",\n",
    "        private_key=\"KafkaSSLPrivateKeyPassword\",\n",
    "        trust_store=\"KafkaSSLTruststorePassword\"\n",
    "    )\n",
    "\n",
    "# Function to read Kafka stream\n",
    "def read_kafka_stream(credentials: KafkaCredentials, starting_offsets: str = \"earliest\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Configure and read data from Kafka as a streaming DataFrame.\n",
    "\n",
    "    Args:\n",
    "        credentials: KafkaCredentials object with username, password, and SSL keys.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A streaming DataFrame from Kafka.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            # Connection Settings\n",
    "            .option(\"kafka.bootstrap.servers\", credentials.broker)\n",
    "            .option(\"subscribe\", credentials.topic)\n",
    "            .option(\"startingOffsets\", starting_offsets)\n",
    "            # Security Settings - Protocol and Mechanism\n",
    "            .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "            .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "            .option(\"kafka.ssl.endpoint.identification.algorithm\", \"\")\n",
    "            # SASL JAAS Credentials\n",
    "            .option(\"kafka.sasl.jaas.config\", f\"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required \n",
    "                                                    username={credentials.username} \n",
    "                                                    password={credentials.password};\"\"\")\n",
    "            # SSL Settings\n",
    "            .option(\"kafka.ssl.truststore.location\", credentials.truststore_location)\n",
    "            .option(\"kafka.ssl.truststore.password\", credentials.truststore_password)\n",
    "            .option(\"kafka.ssl.keystore.location\", credentials.keystore_location)\n",
    "            .option(\"kafka.ssl.keystore.password\", credentials.keystore_password)\n",
    "            .option(\"kafka.ssl.key.password\", credentials.private_key_password)\n",
    "            .load()\n",
    "    )\n",
    "\n",
    "# Function to cast Kafka stream data\n",
    "def cast_kafka_stream_data(kafka_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cast Kafka binary stream data into readable JSON strings.\n",
    "\n",
    "    Args:\n",
    "        kafka_df: The raw Kafka DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Parsed Kafka data with string values.\n",
    "    \"\"\"\n",
    "    return kafka_df.selectExpr(\n",
    "        \"topic\",\n",
    "        \"CAST(value AS STRING) as json_event\",\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        \"timestamp\"\n",
    "    )\n",
    "\n",
    "# Function to write the stream to a Delta table\n",
    "def write_to_delta_stream(df: DataFrame, table_name: str, checkpoint_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Write the streaming DataFrame into a Delta table.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to write.\n",
    "        table_name: Target Delta table name.\n",
    "        checkpoint_path: Path for checkpointing the stream progress.\n",
    "    \"\"\"\n",
    "    df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime=\"1 minute\") \\\n",
    "        .toTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd558936-d286-4955-9f72-570ff510f84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main execution flow\n",
    "def main():\n",
    "    # Retrieve Kafka credentials\n",
    "    creds = retrieve_my_kafka_credentials()\n",
    "    \n",
    "    # Read Kafka stream\n",
    "    kafka_stream_df = read_kafka_stream(creds)\n",
    "    \n",
    "    # Cast Kafka stream data\n",
    "    df_casted = cast_kafka_stream_data(kafka_stream_df)\n",
    "    \n",
    "    # Write stream data to Delta table\n",
    "    write_to_delta_stream(df_casted, bronze_table, checkpoint_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
