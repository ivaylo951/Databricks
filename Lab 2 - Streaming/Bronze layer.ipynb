{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541c22b2-6579-4bba-82a9-04292ea01ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "KAFKA_BROKER = \"detrainingbg.adastragrp.com:9092\"\n",
    "KAFKA_TOPIC = \"marketing.uk.netflix\"\n",
    "KAFKA_SSL_TRUSTSTORE_LOCATION = \"abfss://source-data@sttrwesteu001.dfs.core.windows.net/cert/kafka.truststore.jks\"\n",
    "KAFKA_SSL_KEYSTORE_LOCATION = \"abfss://source-data@sttrwesteu001.dfs.core.windows.net/cert/kafka.keystore.jks\"\n",
    "BRONZE_TABLE = \"bronze.training_io.rt_marketing\"\n",
    "CHECKPOINT_PATH = \"/Volumes/bronze/checkpoints/realtime\"\n",
    "\n",
    "# Function to retrieve secrets\n",
    "def get_secrets(scope, username_key, password_key, key_store, private_key, trust_store):\n",
    "    \"\"\"\n",
    "    Retrieve the Kafka credentials and SSL secrets from Databricks secrets scope.\n",
    "    \"\"\"\n",
    "    username = dbutils.secrets.get(scope, username_key)\n",
    "    password = dbutils.secrets.get(scope, password_key)\n",
    "    key_store = dbutils.secrets.get(scope, key_store)\n",
    "    private_key = dbutils.secrets.get(scope, private_key)\n",
    "    trust_store = dbutils.secrets.get(scope, trust_store)\n",
    "    return username, password, key_store, private_key, trust_store\n",
    "\n",
    "# Retrieve Kafka credentials from secret scope\n",
    "def retrieve_my_kafka_credentials():\n",
    "    \"\"\"\n",
    "    Retrieve Kafka credentials and SSL certificates.\n",
    "    \"\"\"\n",
    "    return get_secrets(\n",
    "        \"dbc-training-scope\", \n",
    "        \"KafkaUsername\", \n",
    "        \"KafkaPassword\", \n",
    "        \"KafkaSSLKeystorePassword\", \n",
    "        \"KafkaSSLPrivateKeyPassword\", \n",
    "        \"KafkaSSLTruststorePassword\"\n",
    "    )\n",
    "\n",
    "# Function to configure Kafka options\n",
    "def configure_kafka_stream(kafka_username, kafka_password, kafka_ssl_keystore_password, kafka_ssl_private_key_password, kafka_ssl_truststore_password):\n",
    "    \"\"\"\n",
    "    Set up Kafka stream options for reading from a Kafka topic.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "        .option(\"subscribe\", KAFKA_TOPIC)\n",
    "        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "        .option(\"kafka.ssl.endpoint.identification.algorithm\", \"\")\n",
    "        .option(\"kafka.sasl.jaas.config\", f\"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required \n",
    "                                                username={kafka_username} \n",
    "                                                password={kafka_password};\"\"\")\n",
    "        .option(\"kafka.ssl.truststore.location\", KAFKA_SSL_TRUSTSTORE_LOCATION)\n",
    "        .option(\"kafka.ssl.truststore.password\", kafka_ssl_truststore_password)\n",
    "        .option(\"kafka.ssl.keystore.location\", KAFKA_SSL_KEYSTORE_LOCATION)\n",
    "        .option(\"kafka.ssl.keystore.password\", kafka_ssl_keystore_password)\n",
    "        .option(\"kafka.ssl.key.password\", kafka_ssl_private_key_password)\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "# Function to cast Kafka stream data\n",
    "def cast_kafka_stream_data(kafka_df):\n",
    "    \"\"\"\n",
    "    Cast the Kafka stream data to json format.\n",
    "    \"\"\"\n",
    "    return kafka_df.selectExpr(\n",
    "        \"topic\",\n",
    "        \"CAST(value AS STRING) as json_event\",\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        \"timestamp\"\n",
    "    )\n",
    "\n",
    "# Function to write the stream to a Delta table\n",
    "def write_to_delta_stream(df, table_name, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Write the stream data to a Delta table.\n",
    "    \"\"\"\n",
    "    df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime=\"1 minute\") \\\n",
    "        .toTable(table_name)\n",
    "\n",
    "# Main execution flow\n",
    "def main():\n",
    "    # Retrieve Kafka credentials\n",
    "    KafkaUsername, KafkaPassword, KafkaSSLKeystorePassword, KafkaSSLPrivateKeyPassword, KafkaSSLTruststorePassword = retrieve_my_kafka_credentials()\n",
    "    \n",
    "    # Configure Kafka stream\n",
    "    kafka_stream_df = configure_kafka_stream(KafkaUsername, KafkaPassword, KafkaSSLKeystorePassword, KafkaSSLPrivateKeyPassword, KafkaSSLTruststorePassword)\n",
    "    \n",
    "    # Cast Kafka stream data\n",
    "    df_casted = cast_kafka_stream_data(kafka_stream_df)\n",
    "    \n",
    "    # Write stream data to Delta table\n",
    "    write_to_delta_stream(df_casted, BRONZE_TABLE, CHECKPOINT_PATH)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
