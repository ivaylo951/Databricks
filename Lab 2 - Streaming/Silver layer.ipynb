{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e414534-17a0-4b02-b584-81be1c219054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, from_json, row_number\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, IntegerType, DateType\n",
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb93338-f3df-40f7-b784-51aa0d6428f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "bronze_table = \"bronze.training_io.rt_marketing\"\n",
    "silver_table = \"silver.training_io.rt_marketing\"\n",
    "checkpoint_path = \"/Volumes/silver/checkpoints/realtime\"\n",
    "key_columns = [\"id\"]\n",
    "timestamp_column = \"kafka_ts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947eb032-7423-42ea-b429-886f738a8a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def flat_bronze_df(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parses the 'json_event' column from the bronze table, flattens the nested JSON structure,\n",
    "    and selects required fields into a flat DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input streaming DataFrame from the bronze table.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Flattened DataFrame with selected fields.\n",
    "    \"\"\"\n",
    "    \n",
    "    schema = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                     StructField(\"datetime\", TimestampType(), True),\n",
    "                     StructField(\"duration\", DoubleType(), True),\n",
    "                     StructField(\"title\", StringType(), True),\n",
    "                     StructField(\"genres\", StringType(), True),\n",
    "                     StructField(\"release_date\", DateType(), True),\n",
    "                     StructField(\"movie_id\", StringType(), True),\n",
    "                     StructField(\"user_id\", StringType(), True)])\n",
    "\n",
    "    df = df.select(\n",
    "        col(\"timestamp\").alias(\"kafka_ts\"),\n",
    "        from_json(col(\"json_event\"), schema=schema).alias(\"parsed_json\")\n",
    "    )\n",
    "\n",
    "    df = df.select(\n",
    "        col(\"kafka_ts\"),\n",
    "        col(\"parsed_json.id\"),\n",
    "        col(\"parsed_json.datetime\"),\n",
    "        col(\"parsed_json.duration\"),\n",
    "        col(\"parsed_json.title\"),\n",
    "        col(\"parsed_json.genres\"),\n",
    "        col(\"parsed_json.release_date\"),\n",
    "        col(\"parsed_json.movie_id\"),\n",
    "        col(\"parsed_json.user_id\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def deduplicate_by_latest_timestamp(df: DataFrame, key_columns: list[str], timestamp_column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Deduplicates the DataFrame by keeping only the latest record per key based on the timestamp.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        key_columns (list[str]): List of column names used as the unique key.\n",
    "        timestamp_column (str): Name of the timestamp column used for ordering.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Deduplicated DataFrame with only the latest records.\n",
    "    \"\"\"\n",
    "\n",
    "    window_spec = Window.partitionBy(*key_columns).orderBy(col(timestamp_column).desc())\n",
    "    return df.withColumn(\"row_num\", row_number().over(window_spec)).filter(\"row_num = 1\").drop(\"row_num\")\n",
    "\n",
    "def upsert(microBatchDF: DataFrame, batchId: int, silver_table: str, key_columns: list[str], timestamp_column: str) -> None:\n",
    "    \"\"\"\n",
    "    Performs an SCD1-style upsert (merge) from a microbatch DataFrame into a Silver Delta table.\n",
    "    Deduplication is performed before the merge.\n",
    "\n",
    "    Args:\n",
    "        microBatchDF (DataFrame): Microbatch DataFrame provided by foreachBatch.\n",
    "        batchId (int): ID of the microbatch provided by foreachBatch.\n",
    "        silver_table (str): Name of the target Silver Delta table.\n",
    "        key_columns (list[str]): List of column names used for joining (keys).\n",
    "        timestamp_column (str): Name of the timestamp column used to resolve conflicts.\n",
    "    \"\"\"\n",
    "\n",
    "    microBatchDF = deduplicate_by_latest_timestamp(microBatchDF, key_columns, timestamp_column)\n",
    "\n",
    "    silver_delta_table = DeltaTable.forName(spark, silver_table)\n",
    "\n",
    "    join_condition = \" AND \".join([f\"target.{col} = source.{col}\" for col in key_columns])\n",
    "    update_set = {col: f\"source.{col}\" for col in microBatchDF.columns}\n",
    "\n",
    "    silver_delta_table.alias(\"target\").merge(\n",
    "        microBatchDF.alias(\"source\"),\n",
    "        join_condition\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=f\"source.{timestamp_column} > target.{timestamp_column}\",\n",
    "        set=update_set\n",
    "    ).whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1ea0b7-16b6-4e90-8405-5a48153d0ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read the data from the bronze table\n",
    "    bronze_df = spark.readStream.table(bronze_table)\n",
    "\n",
    "    # Flat the json data\n",
    "    bronze_df = flat_bronze_df(bronze_df)\n",
    "\n",
    "    # Start streaming with foreachBatch for upserts\n",
    "    bronze_df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .foreachBatch(lambda df, batchId: upsert(df, batchId, silver_table, key_columns, timestamp_column)) \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .trigger(processingTime=\"1 minute\") \\\n",
    "        .start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4511171444221275,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
